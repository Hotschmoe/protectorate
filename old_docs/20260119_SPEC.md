# Cortical Stacks - Architecture Spec v2.2
> Date: 2026-01-19
> Status: ALL DECISIONS LOCKED - Ready for Implementation

---

## Table of Contents

1. [Project Ethos](#project-ethos)
2. [Repository Strategy](#repository-strategy)
3. [Naming Decisions](#naming-decisions)
4. [Memory and Communication Layer](#memory-and-communication-layer)
5. [Bootstrap and Manager Architecture](#bootstrap-and-manager-architecture)
6. [Multi-Machine Topology](#multi-machine-topology)
7. [Local Git Infrastructure](#local-git-infrastructure)
8. [Container Architecture](#container-architecture)
9. [UI and Setup Wizard](#ui-and-setup-wizard)
10. [Tooling and Forks](#tooling-and-forks)
11. [Answered Questions Summary](#answered-questions-summary)
12. [V1 Scope](#v1-scope)
13. [V2 Roadmap](#v2-roadmap)
14. [Implementation Phases](#implementation-phases)
15. [Open Questions and Discussion Items](#open-questions-and-discussion-items)

---

## Project Ethos

### Core Philosophy: Native CLI Tools, Managed Orchestration

```
+----------------------------------------------------------------------+
|                         PROJECT ETHOS                                 |
+----------------------------------------------------------------------+

WE DO NOT:
  - Create brand new AI harnesses
  - Wrap or modify how Claude Code, Gemini CLI, etc. work internally
  - Fight against how these tools evolve

WE DO:
  - Let Anthropic, Google, OpenAI update and refine their CLI tools
  - Allow one person to manage DOZENS of these harnesses
  - Provide orchestration, memory persistence, and coordination
  - Defer to native solutions when they ship better alternatives

ANALOGY:
  - CLI tools (Claude Code, etc.) = Consciousness
  - Our containers = Sleeves (bodies)
  - Our orchestration = The network that coordinates sleeves
  - We don't modify the consciousness, we manage the sleeves

+----------------------------------------------------------------------+
```

### Fork and Maintain Strategy

All external tools we depend on will be:
1. **Forked** to our organization
2. **Maintained** as stable versions
3. **Updated** by pulling changes from upstream as needed
4. **Replaced** if the original tool ships something better

This protects us from:
- Upstream breaking changes
- Abandoned projects
- API drift
- Feature removal

---

## Repository Strategy

### Decision: Fresh Start with Two Repositories

```
+----------------------------------------------------------------------+
|                      REPOSITORY STRUCTURE                             |
+----------------------------------------------------------------------+

REPO 1: cortical-stack (Memory/State)
+----------------------------------+
| Purpose: Agent memory and state  |
| Contains:                        |
|   - .cstack/ markdown files      |
|   - Stack file parsers/writers   |
|   - May evolve to beads fork     |
|   - Install scripts for agents   |
| Standalone: YES (project agnostic)|
+----------------------------------+

REPO 2: protectorate (Orchestration)
+----------------------------------+
| Purpose: Sleeve orchestration    |
| Contains:                        |
|   - containers/agent/            |
|   - containers/envoy/ (manager)  |
|   - cmd/envoy/ (CLI)             |
|   - cmd/sidecar/                 |
|   - Bootstrap logic              |
|   - API definitions              |
|   - UI/wizard code               |
+----------------------------------+

CLI TOOL: "envoy" (built from protectorate repo)
  envoy spawn --repo foo --cli claude-code
  envoy status
  envoy resleeve agent-alice --cli gemini

RATIONALE:
- Clear separation of concerns
- Stack repo can be pulled into any agent independently
- Stack repo is project-agnostic (anyone can use it)
- Orchestration changes don't touch memory format
- Enables stack repo to evolve (potentially to beads fork)
- Each repo has its own versioning (SemVer)

+----------------------------------------------------------------------+
```

### Disposition of Current Code

The current `cortical-stacks` repo contains ~1,500 lines of Go code. We will:
1. Archive this repo (or keep as reference)
2. Start fresh with the two-repo structure
3. Selectively port code that aligns with new architecture
4. Do NOT attempt to migrate/refactor in place

---

## Naming Decisions

### Orchestration Repo and CLI Names

```
+----------------------------------------------------------------------+
|                      NAMING DECISIONS - LOCKED                        |
+----------------------------------------------------------------------+

FRAMEWORK/REPO: "Protectorate"
  - The governing body in Altered Carbon
  - Oversees and controls all sleeves
  - Repository: github.com/[org]/protectorate

CLI TOOL: "envoy"
  - "One who carries messages between parties"
  - The manager process that coordinates sleeves
  - Built from protectorate repo, binary named "envoy"

TERMINOLOGY:
  +----------------------------------------------------------+
  | Term          | Meaning                                   |
  |---------------|-------------------------------------------|
  | Protectorate  | The framework/system (repo name)          |
  | Envoy         | The manager process/CLI tool              |
  | Sleeve        | Agent container (the body)                |
  | Cortical Stack| Agent memory format (.cstack/)            |
  | DHF           | The AI consciousness (Claude, Gemini, etc)|
  | Resleeve      | Switch CLI or respawn container           |
  +----------------------------------------------------------+

WHY THIS NAMING:
  - "Protectorate" is more evocative of Altered Carbon
  - "Envoy" as the CLI/process is short and fits the role
  - Clear distinction: Protectorate = system, Envoy = tool
  - Fun naming scheme that maintains thematic consistency

+----------------------------------------------------------------------+
```

### Agent Naming: Pre-made List

Agents will receive memorable names from a pre-made list (not generated or sequential).

```
+----------------------------------------------------------------------+
|                      AGENT NAMING STRATEGY                            |
+----------------------------------------------------------------------+

APPROACH: Curated list of memorable names
  - Themed (sci-fi, mythology, etc.)
  - Easy to remember and reference
  - No collisions within a deployment

SOURCES FOR NAME LIST:
  - Altered Carbon characters
  - Greek/Roman deities
  - Sci-fi AI names
  - Constellation names

IMPLEMENTATION:
  - names.yaml in config repo
  - Manager assigns next available name on spawn
  - Name returned to pool on agent destruction
  - User can override with custom name if desired

EXAMPLE LIST:
  quell, virginia, rei, mickey, trepp, tanaka,
  athena, apollo, hermes, iris, prometheus,
  hal, samantha, jarvis, cortana, shodan

+----------------------------------------------------------------------+
```

---

## Memory and Communication Layer

### Decision: Markdown Stack Files for V1

```
+----------------------------------------------------------------------+
|                    V1: MARKDOWN STACK FILES                           |
+----------------------------------------------------------------------+

STRUCTURE:
  repo/
  +-- .cstack/
      +-- CURRENT.md    # Active task, focus, next steps
      +-- PLAN.md       # Task backlog with states
      +-- INBOX.md      # Messages TO this agent
      +-- OUTBOX.md     # Messages FROM this agent
      +-- CONTEXT.md    # Long-term context/memory (optional)

WHY MARKDOWN FOR V1:
  1. Single directory = single concern
  2. Human readable and editable
  3. Git-native (no database)
  4. Perfect portability (survives nuke and restore)
  5. Text editor is the debugger
  6. Faster to implement than beads integration

TASK FORMAT (PLAN.md):
  - [ ] Pending task
  - [>] In progress task
  - [x] Completed task
  - [!] Blocked task

MESSAGE FORMAT (INBOX/OUTBOX):
  ---
  ID: msg-a7f3
  From: agent-id
  To: agent-id (or "manager")
  Thread: optional-thread-id
  Type: task|question|milestone|blocked|done
  Time: 2026-01-19T10:30:00Z
  ---
  Message content here.

ENHANCEMENTS FROM BEADS (Borrowed Ideas):
  - Hash-based message IDs (msg-xxxx) for deduplication
  - Thread IDs for conversation grouping
  - Parent/child task notation (future)

+----------------------------------------------------------------------+
```

### Stack Repository Evolution Path

```
+----------------------------------------------------------------------+
|                    STACK EVOLUTION PATH                               |
+----------------------------------------------------------------------+

V1 (Current):
  - Pure markdown files in .cstack/
  - Simple, portable, human-readable
  - No external dependencies

V2 (Potential):
  - Fork beads_rust: github.com/Dicklesworthstone/beads_rust
  - Maintain as our own stable version
  - Integrate beads-style features:
    - Dependency-aware task graph
    - Semantic compaction
    - Better search
  - Stack repo becomes our beads fork

V3+ (Adaptive):
  - IF Claude Code ships native memory persistence
  - THEN we evaluate and potentially defer to it
  - Our ethos: don't fight native solutions

DECISION TRIGGER FOR V2:
  - When markdown grep becomes painful
  - When task dependencies become complex
  - When semantic compaction is needed

+----------------------------------------------------------------------+
```

### Communication: Markdown INBOX/OUTBOX

```
+----------------------------------------------------------------------+
|                    INTER-AGENT COMMUNICATION                          |
+----------------------------------------------------------------------+

V1 APPROACH: Markdown INBOX/OUTBOX files

HOW IT WORKS:
  1. Agent A writes message to own OUTBOX.md
  2. Manager reads all agent OUTBOX files on polling cycle
  3. Manager routes message to Agent B's INBOX.md
  4. Agent B reads INBOX.md on next cycle
  5. Manager clears processed messages from OUTBOX

MANAGER AS ROUTER:
  +----------+        +----------+        +----------+
  | Agent A  |        | Manager  |        | Agent B  |
  | OUTBOX   |------->|  Routes  |------->| INBOX    |
  +----------+        +----------+        +----------+

BENEFITS:
  - No additional infrastructure
  - Messages persisted in git
  - Human can inspect/inject messages
  - Works with git-only portability

FUTURE (V2):
  - Consider agent_mail for advanced features
  - Consider direct agent-to-agent for latency
  - Consider pub/sub for broadcast messages

+----------------------------------------------------------------------+
```

---

## Bootstrap and Manager Architecture

### Decision: Manager Container IS the Bootstrapper

```
+----------------------------------------------------------------------+
|              MANAGER AS BOOTSTRAPPER ARCHITECTURE                     |
+----------------------------------------------------------------------+

SINGLE CONTAINER, TWO MODES:

  +--------------------------------------------------+
  |              MANAGER CONTAINER                    |
  |                                                   |
  |  +-------------------+   +-------------------+   |
  |  | BOOTSTRAP MODE    |   | MANAGER MODE      |   |
  |  | (First run)       |   | (Normal operation)|   |
  |  |                   |   |                   |   |
  |  | - Check env       |   | - Spawn agents    |   |
  |  | - Setup wizard    |   | - Route messages  |   |
  |  | - Create Gitea    |   | - Health checks   |   |
  |  | - Init config     |   | - API server      |   |
  |  | - Create networks |   | - UI server       |   |
  |  +-------------------+   +-------------------+   |
  |           |                                      |
  |           +---> Transitions to manager mode ---->|
  +--------------------------------------------------+

DETECTION LOGIC:
  if CORTICAL_BOOTSTRAP=true OR first_run_detected:
      run_bootstrap_mode()
  run_manager_mode()

FIRST RUN DETECTION:
  - Check for existence of config repo in Gitea
  - Check for existence of cortical-config volume
  - If neither exists, this is first run

BOOTSTRAP SEQUENCE:
  1. Display setup wizard (TUI)
  2. Collect API keys, domain config
  3. Create Docker networks
  4. Spawn Gitea container
  5. Initialize cortical-config repo
  6. Write collected config to repo
  7. Create .env or secrets file
  8. Transition to manager mode

+----------------------------------------------------------------------+
```

### Manager Startup Flow

```
+----------------------------------------------------------------------+
|                    MANAGER STARTUP FLOW                               |
+----------------------------------------------------------------------+

docker run cortical-manager
        |
        v
+-------------------+
| Check environment |
| and existing state|
+-------+-----------+
        |
        v
+-------+-------+
| First run?    |
+---+-------+---+
    |       |
   YES      NO
    |       |
    v       v
+-------+  +----------------+
|WIZARD |  | Load config    |
|SETUP  |  | from Gitea     |
+---+---+  +-------+--------+
    |              |
    v              |
+----------+       |
| Bootstrap|       |
| Gitea,   |       |
| networks,|       |
| config   |       |
+----+-----+       |
     |             |
     +------+------+
            |
            v
    +-------+-------+
    | MANAGER MODE  |
    | - API server  |
    | - UI server   |
    | - Poll agents |
    | - Route msgs  |
    +---------------+

+----------------------------------------------------------------------+
```

---

## Multi-Machine Topology

### Decision: MASTER/SLAVE Architecture (V2 Stretch Goal)

```
+----------------------------------------------------------------------+
|              MULTI-MACHINE: MASTER/SLAVE TOPOLOGY                     |
+----------------------------------------------------------------------+

V1: Single machine, single manager

V2: Multiple machines with hierarchy

ARCHITECTURE:
                        +------------------+
                        |  MASTER MANAGER  |
                        |  (Primary host)  |
                        |  + GITEA         |
                        +--------+---------+
                                 |
              +------------------+------------------+
              |                  |                  |
              v                  v                  v
      +-------+------+   +------+-------+   +------+-------+
      | SLAVE MGR    |   | SLAVE MGR    |   | SLAVE MGR    |
      | (Host B)     |   | (Host C)     |   | (Host D)     |
      |              |   |              |   |              |
      | Agents 1-5   |   | Agents 6-10  |   | Agents 11-15 |
      +--------------+   +--------------+   +--------------+

DOMAIN-DRIVEN ADDRESSING:
  - Master: cortical.hotschmoe.com (example)
  - Gitea: gitea.hotschmoe.com
  - Slaves: Referenced by master, not directly addressed

SETUP WIZARD OPTIONS:
  1) Start Cortical (become MASTER)
  2) Join Cortical (become SLAVE)

MASTER RESPONSIBILITIES:
  - Run Gitea (or delegate to dedicated host)
  - Coordinate all managers
  - Route cross-host messages
  - Central configuration
  - Mirror to GitHub

SLAVE RESPONSIBILITIES:
  - Spawn agents on local host only
  - Report to master
  - Execute master's commands
  - Local Docker socket access only

COMMUNICATION:
  - Master <-> Slave: HTTP API
  - All containers reach Gitea by domain name
  - Cross-host agent messages routed via masters

+----------------------------------------------------------------------+

NOTE: This is a STRETCH GOAL for V2. V1 is single-machine only.
Design decisions for V1 should not preclude this architecture.

+----------------------------------------------------------------------+
```

---

## Local Git Infrastructure

### Decision: Gitea with Daily Push Mirror

```
+----------------------------------------------------------------------+
|                    GITEA CONFIGURATION                                |
+----------------------------------------------------------------------+

CHOICE: Gitea (not GitLab, not Forgejo)

RATIONALE:
  - Lightweight (~100MB RAM)
  - Go-based (matches our stack)
  - GitHub Actions compatible
  - Built-in container registry
  - Easy backup (SQLite + git repos)

DEPLOYMENT:
  gitea:
    image: gitea/gitea:latest
    container_name: cortical-gitea
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - GITEA__server__ROOT_URL=http://gitea:3000
      - GITEA__repository__DEFAULT_PRIVATE=true
    volumes:
      - ${CORTICAL_DATA}/gitea:/data
    ports:
      - "3000:3000"
      - "2222:22"
    networks:
      - cortical-net
    restart: unless-stopped

SPAWNED BY: Manager in bootstrap mode

+----------------------------------------------------------------------+
```

### Mirror Strategy

```
+----------------------------------------------------------------------+
|                    GITHUB MIRROR STRATEGY                             |
+----------------------------------------------------------------------+

FREQUENCY: Daily (cron job)

MECHANISM:
  - Manager runs daily cron task
  - For each repo in cortical-config/repos.yaml:
    - git push --mirror to GitHub remote
  - Protected branches on GitHub (no force push, no delete)

CONFIGURATION (cortical-config/mirrors.yaml):
  mirrors:
    - source: gitea/org/repo-foo
      target: github/org/repo-foo
      frequency: daily
      protected: true
    - source: gitea/org/cortical-config
      target: github/org/cortical-config
      frequency: daily
      protected: true

EDGE CASES:

  Q: What if repo exists only locally (no GitHub mirror target)?
  A: Options:
     1. Skip mirroring for that repo (log warning)
     2. Auto-create repo on GitHub (if token has scope)
     3. Prompt user to create mirror target

  RECOMMENDATION: Option 2 (auto-create) with fallback to Option 1

+----------------------------------------------------------------------+
```

### Daily Summary Agent (Additional Feature)

```
+----------------------------------------------------------------------+
|                    DAILY SUMMARY AGENT                                |
+----------------------------------------------------------------------+

TRIGGER: Same cron as mirror (once daily)

FUNCTION:
  1. Review git history for all repos worked on that day
  2. Generate two outputs per repo:
     a. TL;DR Summary (concise, what changed)
     b. Changelog (verbose but fluff-filtered)
  3. Commit summaries to cortical-config repo
  4. Optionally notify user (V2: messaging integration)

OUTPUT LOCATION:
  cortical-config/
  +-- daily-summaries/
      +-- 2026-01-19/
          +-- repo-foo-summary.md
          +-- repo-foo-changelog.md
          +-- repo-bar-summary.md
          +-- repo-bar-changelog.md

IMPLEMENTATION: Dedicated "summary-agent" persona
  - Uses same agent container image
  - Specialized stack/instructions for summarization
  - Read-only access to all repos

+----------------------------------------------------------------------+
```

---

## Container Architecture

### Decision: Debian Bookworm Slim Base

```
+----------------------------------------------------------------------+
|                    CONTAINER BASE IMAGE                               |
+----------------------------------------------------------------------+

CHOICE: debian:bookworm-slim

RATIONALE:
  - Smaller than Ubuntu (~80MB vs ~180MB base)
  - More stable than Alpine (glibc vs musl)
  - Better compatibility with AI CLI tools
  - Widely supported, well-documented
  - Security updates from Debian team

AGENT IMAGE STRUCTURE:
  FROM debian:bookworm-slim

  # System dependencies
  RUN apt-get update && apt-get install -y \
      curl git nodejs npm python3 \
      && rm -rf /var/lib/apt/lists/*

  # AI CLI tools (ALL in one image for V1)
  RUN npm install -g @anthropic/claude-code
  # Add other CLIs as needed

  # Sidecar binary
  COPY sidecar /usr/local/bin/sidecar

  # Stack directory
  RUN mkdir -p /workspace/.cstack

  ENTRYPOINT ["/usr/local/bin/sidecar"]

+----------------------------------------------------------------------+
```

### Single Agent Image Strategy (V1)

```
+----------------------------------------------------------------------+
|                    SINGLE AGENT IMAGE (V1)                            |
+----------------------------------------------------------------------+

V1 APPROACH: One image with ALL CLI tools

IMAGE CONTENTS:
  - Base: Debian Bookworm Slim
  - Runtime: Node.js, Python3, Go (for tools)
  - AI CLIs: Claude Code, OpenCode, Gemini CLI, etc.
  - Sidecar: Our health/status server
  - Stack tools: Markdown parsers, etc.

PERSONA DIFFERENTIATION:
  - Same image, different .cstack/ contents
  - CURRENT.md defines active persona
  - CLAUDE.md (or equivalent) provides instructions
  - Environment variables can hint persona

BENEFITS:
  - One image to maintain
  - Any container can become any persona
  - Simpler CI/CD

FUTURE (V2):
  - Consider splitting if images become too large
  - Consider per-CLI images if tools conflict
  - Consider minimal images for resource-constrained hosts

+----------------------------------------------------------------------+
```

---

## UI and Setup Wizard

### Decision: Manager Container Hosts UI (Minimal for V1)

```
+----------------------------------------------------------------------+
|                    UI ARCHITECTURE                                    |
+----------------------------------------------------------------------+

DECISION: UI lives IN the manager container (not separate)

RATIONALE:
  - Security: No separate container with manager access
  - Simplicity: One less container to manage
  - Tightly coupled: UI and manager share state naturally

V1 UI REQUIREMENTS (Minimal):
  1. Setup wizard (first run)
     - Collect API keys
     - Configure domain
     - Initialize system
  2. Dashboard
     - List all containers on host
     - Show agent status (from sidecar)
     - Show stack summaries
  3. Manager interaction
     - Chat window to manager
     - Send commands
     - View responses

V1 UI TECHNOLOGY OPTIONS:
  a) TUI (Terminal UI) - bubbletea, tview
     - Works over SSH
     - No browser needed
     - Fits "text-based" requirement

  b) Simple Web UI - HTML + minimal JS
     - Accessible from any device
     - Can be very minimal
     - ASCII art styling possible

RECOMMENDATION: Start with TUI, add simple web later

+----------------------------------------------------------------------+
```

### Setup Wizard Flow

```
+----------------------------------------------------------------------+
|                    SETUP WIZARD FLOW                                  |
+----------------------------------------------------------------------+

TRIGGER: First run of manager container

SCREENS:

1. WELCOME
   +------------------------------------------+
   |                                          |
   |   CORTICAL SETUP                         |
   |                                          |
   |   Welcome to the Cortical orchestration  |
   |   system. This wizard will help you      |
   |   configure your environment.            |
   |                                          |
   |   [1] Start new Cortical deployment      |
   |   [2] Join existing Cortical deployment  |
   |                                          |
   +------------------------------------------+

2. API KEYS (if option 1)
   +------------------------------------------+
   |   API CONFIGURATION                      |
   |                                          |
   |   Enter your API keys below.             |
   |   These will be stored securely.         |
   |                                          |
   |   Anthropic API Key: [_______________]   |
   |   OpenRouter Key:    [_______________]   |
   |   GitHub Token:      [_______________]   |
   |                                          |
   |   [Continue]                             |
   +------------------------------------------+

3. DOMAIN CONFIG
   +------------------------------------------+
   |   NETWORK CONFIGURATION                  |
   |                                          |
   |   Domain: [cortical.local___________]    |
   |                                          |
   |   This domain will be used for:          |
   |   - gitea.{domain} - Git server          |
   |   - manager.{domain} - Manager API       |
   |                                          |
   |   [Continue]                             |
   +------------------------------------------+

4. CONFIRMATION
   +------------------------------------------+
   |   CONFIRM SETUP                          |
   |                                          |
   |   Domain: cortical.local                 |
   |   API Keys: Configured                   |
   |   Mode: MASTER                           |
   |                                          |
   |   The following will be created:         |
   |   - Gitea container                      |
   |   - cortical-net network                 |
   |   - Config repository                    |
   |                                          |
   |   [Start Setup]  [Back]                  |
   +------------------------------------------+

OUTPUT:
  - .env file OR encrypted secrets file
  - cortical-config repo initialized
  - Gitea spawned and configured
  - Manager transitions to normal mode

+----------------------------------------------------------------------+
```

---

## Tooling and Forks

### Tools to Fork and Maintain

```
+----------------------------------------------------------------------+
|                    FORKED TOOLS                                       |
+----------------------------------------------------------------------+

TOOL: beads_rust
  SOURCE: github.com/Dicklesworthstone/beads_rust
  PURPOSE: Future memory/task management (V2+)
  STATUS: Fork when ready to integrate, not before

TOOL: qmd (QMD Search)
  SOURCE: github.com/tobi/qmd
  PURPOSE: Markdown search for manager
  USE CASE: Manager searches all agent .cstack/ directories
  STATUS: Evaluate and fork for V1

TOOL: rch (Remote Compilation Helper)
  SOURCE: github.com/Dicklesworthstone/remote_compilation_helper
  PURPOSE: Remote building on "fat build" containers
  STATUS: V2 feature, fork when needed

FORKING PROCESS:
  1. Fork to our GitHub org
  2. Create stable branch
  3. Document any modifications
  4. Set up upstream tracking
  5. Periodic sync from upstream (manual review)

+----------------------------------------------------------------------+
```

### QMD Integration (V1)

```
+----------------------------------------------------------------------+
|                    QMD SEARCH INTEGRATION                             |
+----------------------------------------------------------------------+

PURPOSE: Enable manager to search markdown stacks efficiently

USE CASES:
  1. Find all blocked tasks across all agents
  2. Search for specific keywords in agent states
  3. Find messages mentioning specific topics
  4. Audit agent activity

INTEGRATION:
  - Fork qmd to our org
  - Build into manager container
  - Manager exposes search API endpoint
  - UI provides search interface

EXAMPLE QUERIES:
  - "blocked" -> Find all blocked tasks
  - "from:agent-alice" -> Messages from specific agent
  - "type:milestone" -> All milestone notifications

+----------------------------------------------------------------------+
```

---

## Answered Questions Summary

```
+----------------------------------------------------------------------+
|                    DECISION SUMMARY (Updated v2.2)                    |
+----------------------------------------------------------------------+

ORIGINAL DECISIONS (v2.0):
| # | Question | Decision | Version |
|---|----------|----------|---------|
| Q1 | Agent naming | Pre-made list | V1 |
| Q2 | Stack location | Separate repo (cortical-stack) | V1 |
| Q3 | UI | In manager container, minimal TUI | V1 |
| Q4 | Messaging integration | None | V2 |
| Q5 | Warm container pool | No | V2 |
| Q6 | Agent deployment to prod | CI only | V2 |
| Q7 | Version scheme | SemVer for all | V1 |
| Q8 | Image hosting | Both local (Gitea) + backup (ghcr.io) | V1 |
| Q9 | Mirror frequency | Daily | V1 |
| Q10 | Stack auto-commit | On milestone | V1 |
| Q11 | Agent timeout | Configurable, default never | V1 |
| Q12 | Gitea backup | Both (volume + git bundle) | V1 |
| Q13 | Existing code | Fresh repos (cortical-stack + protectorate) | V1 |
| Q14 | Container base | Debian Bookworm Slim | V1 |
| Q15 | CLI images | Single image with all CLIs + languages | V1 |

ADDITIONAL DECISIONS (v2.2):
| # | Question | Decision | Version |
|---|----------|----------|---------|
| Q16 | CLI auth | Host auth inheritance (~/.claude/) | V1 |
| Q17 | Repo naming | Protectorate (repo), Envoy (CLI) | V1 |
| Q18 | cortical-stack | Project-agnostic, standalone | V1 |
| Q19 | Agent Docker | Manager proxy (Options B/C fallback) | V1 |
| Q20 | Languages | Python, Node, Bun, Rust, Zig | V1 |
| Q21 | Multi-agent/repo | DISALLOWED (one sleeve per stack) | V1 |
| Q22 | Resleeve types | Soft (CLI swap) + Hard (respawn) | V1 |
| Q23 | Reverse proxy | Deferred, research early | V2 |
| Q24 | Ralphing/loops | Deferred, needs research | V2 |
| Q25 | Workspace model | Confirmed, keep /shared/arena | V1 |

+----------------------------------------------------------------------+
```

---

## V1 Scope

### What V1 Includes

```
+----------------------------------------------------------------------+
|                    V1 SCOPE: INCLUDED                                 |
+----------------------------------------------------------------------+

REPOSITORIES:
  [x] cortical-stack repo (markdown stack files, project-agnostic)
  [x] protectorate repo (envoy manager, containers, CLI)

CONTAINERS:
  [x] Envoy manager container (with bootstrap mode)
  [x] Gitea container (spawned by envoy)
  [x] Agent container (single image, all CLIs + languages)

FEATURES:
  [x] Setup wizard (TUI in manager)
  [x] Markdown stack files (.cstack/ directory)
  [x] INBOX/OUTBOX message routing
  [x] Agent spawning and lifecycle
  [x] Sidecar health/status API
  [x] Daily GitHub mirror (cron)
  [x] Daily summary agent
  [x] QMD search integration
  [x] Minimal dashboard UI
  [x] Soft resleeve (CLI swap)
  [x] Hard resleeve (container respawn)
  [x] Docker proxy for agent testing (via manager)

INFRASTRUCTURE:
  [x] Single host deployment
  [x] Single manager (envoy)
  [x] Gitea for local git
  [x] Docker networking
  [x] /shared/arena/ mounted (unused until V2)

CONFIGURATION:
  [x] SemVer versioning
  [x] Debian Bookworm Slim base
  [x] Pre-installed: Python, Node, Bun, Rust, Zig
  [x] Pre-made agent name list
  [x] Configurable timeouts
  [x] Host auth inheritance for AI CLIs

POLICIES:
  [x] One sleeve per stack (strictly enforced)
  [x] cortical-stack is project-agnostic

+----------------------------------------------------------------------+
```

### What V1 Excludes (Deferred to V2+)

```
+----------------------------------------------------------------------+
|                    V1 SCOPE: EXCLUDED (V2+)                           |
+----------------------------------------------------------------------+

V2 FEATURES (Next Major Version):
  [ ] Multi-machine (MASTER/SLAVE topology)
  [ ] Traefik reverse proxy + auto service discovery
  [ ] Ralphing / agent loops (needs research first)
  [ ] Shared arena messaging (/shared/arena/ usage)
  [ ] Messaging integration (Telegram/Slack/Matrix)
  [ ] Warm container pool
  [ ] Agent deployment to production
  [ ] Beads integration (evolved memory)
  [ ] Per-CLI container images
  [ ] Remote compilation (rch)
  [ ] Advanced UI (web-based)

V3+ FEATURES (Future):
  [ ] babeld/mesh networking
  [ ] Grafana metrics
  [ ] Federation between deployments

STRETCH GOALS (May pull into V1 if time):
  [ ] GitHub OAuth for repo import
  [ ] Web UI (in addition to TUI)

RESEARCH REQUIRED BEFORE V2:
  [ ] Deep dive https://ghuntley.com/loop/ for ralphing
  [ ] Traefik Docker provider patterns
  [ ] Multi-host networking options

+----------------------------------------------------------------------+
```

---

## V2 Roadmap

### Features Planned for V2

```
+----------------------------------------------------------------------+
|                    V2 ROADMAP                                         |
+----------------------------------------------------------------------+

MULTI-MACHINE:
  - MASTER/SLAVE manager topology
  - Domain-driven container addressing
  - Cross-host message routing
  - Distributed agent spawning

MESSAGING INTEGRATION:
  - Priority: Telegram or self-hosted (easiest first)
  - Notifications on milestones
  - Remote commands via chat
  - Status updates

ADVANCED MEMORY:
  - Evaluate beads_rust fork
  - Dependency-aware tasks
  - Semantic compaction
  - Better search

CONTAINER OPTIMIZATION:
  - Warm container pool
  - Per-CLI images (if needed)
  - Resource-aware scheduling

DEPLOYMENT:
  - Agent-driven prod deployment
  - CI/CD pipeline integration
  - Canary deployments

REMOTE BUILD:
  - rch integration
  - Dedicated build containers
  - Cross-platform builds

+----------------------------------------------------------------------+
```

---

## Implementation Phases

### Phase 1: Foundation (Repos and Core)

```
+----------------------------------------------------------------------+
|                    PHASE 1: FOUNDATION                                |
+----------------------------------------------------------------------+

DURATION: [TBD]

DELIVERABLES:
  1. Create cortical-stack repo
     - Basic .cstack/ file structure
     - Markdown parsers (Go)
     - Documentation

  2. Create orchestration repo
     - Project structure
     - Basic manager skeleton
     - Dockerfile definitions

  3. Container images
     - Manager image (Debian Bookworm Slim)
     - Agent image (all CLIs)
     - Push to ghcr.io

  4. Docker infrastructure
     - Network definitions
     - Volume mounts
     - docker-compose.yml

VALIDATION:
  - Can build both repos
  - Can build container images
  - Images run and respond to health checks

+----------------------------------------------------------------------+
```

### Phase 2: Bootstrap and Gitea

```
+----------------------------------------------------------------------+
|                    PHASE 2: BOOTSTRAP AND GITEA                       |
+----------------------------------------------------------------------+

DURATION: [TBD]

DELIVERABLES:
  1. Setup wizard
     - TUI implementation
     - API key collection
     - Configuration storage

  2. Bootstrap mode
     - First-run detection
     - Gitea spawning
     - Network creation
     - Config repo initialization

  3. Gitea integration
     - Service account creation
     - Token generation
     - Repo creation API

VALIDATION:
  - Fresh docker run triggers wizard
  - Wizard collects config and spawns Gitea
  - Manager transitions to normal mode
  - Config persisted in Gitea repo

+----------------------------------------------------------------------+
```

### Phase 3: Agent Lifecycle

```
+----------------------------------------------------------------------+
|                    PHASE 3: AGENT LIFECYCLE                           |
+----------------------------------------------------------------------+

DURATION: [TBD]

DELIVERABLES:
  1. Agent spawning
     - Spawn API endpoint
     - Container creation
     - Workspace mounting
     - Stack initialization

  2. Sidecar
     - Health endpoint
     - Status endpoint
     - Stack file parsing

  3. Lifecycle management
     - Polling loop
     - State tracking
     - Timeout handling

VALIDATION:
  - Can spawn agent via API
  - Agent reports health
  - Agent status visible in manager
  - Agent timeout works

+----------------------------------------------------------------------+
```

### Phase 4: Communication and UI

```
+----------------------------------------------------------------------+
|                    PHASE 4: COMMUNICATION AND UI                      |
+----------------------------------------------------------------------+

DURATION: [TBD]

DELIVERABLES:
  1. Message routing
     - OUTBOX polling
     - INBOX writing
     - Manager-to-agent messages

  2. Dashboard UI
     - Container list
     - Agent status
     - Stack summaries

  3. Manager chat
     - Command interface
     - Response display

VALIDATION:
  - Agent A can message Agent B via manager
  - Dashboard shows all agents
  - Can chat with manager

+----------------------------------------------------------------------+
```

### Phase 5: Mirror and Summary

```
+----------------------------------------------------------------------+
|                    PHASE 5: MIRROR AND SUMMARY                        |
+----------------------------------------------------------------------+

DURATION: [TBD]

DELIVERABLES:
  1. GitHub mirror
     - Cron job setup
     - Push mirror implementation
     - Protected branch handling

  2. Summary agent
     - Git history analysis
     - TL;DR generation
     - Changelog generation

  3. QMD integration
     - Fork and integrate
     - Search API
     - UI search

VALIDATION:
  - Daily mirror runs successfully
  - Summaries generated and committed
  - Can search across all stacks

+----------------------------------------------------------------------+
```

---

## Appendix: GitHub Repo Import

### How Users Import Existing Repos (V1)

```
+----------------------------------------------------------------------+
|                    GITHUB REPO IMPORT                                 |
+----------------------------------------------------------------------+

PROBLEM: Users have existing repos on GitHub they want to work on

OPTIONS:

OPTION A: OAuth Login
  - User logs into GitHub via OAuth
  - UI shows their repos
  - User selects repos to import
  - Manager clones to Gitea

OPTION B: Prompt on Agent Spawn
  - When spawning agent, prompt for repo URL
  - Manager clones from GitHub to Gitea
  - Creates mirror relationship

OPTION C: Setup Wizard List
  - During setup, user provides list of repos
  - Manager imports all during bootstrap
  - Additional repos added via UI/API later

OPTION D: Manual (V1 Simple)
  - User manually clones to Gitea
  - User registers repo with manager
  - Most manual but simplest to implement

DECISION FOR V1: Option D (Manual)
  - User imports repos via Gitea web UI (gitea:3000)
  - Gitea supports: create new, import from URL, mirror setup
  - User registers repo with manager after import
  - Simplest to implement, Gitea handles the complexity
  - OAuth and wizard-based import deferred to V2

EDGE CASE: Creating New Repos
  - When repo doesn't exist anywhere
  - Manager creates in Gitea
  - On first mirror, creates on GitHub (if token has scope)

+----------------------------------------------------------------------+
```

---

## Open Questions and Discussion Items

> These items need resolution before or during implementation.

### Q1: Claude Code Authentication in Containers

```
+----------------------------------------------------------------------+
|              CLAUDE CODE AUTH - RESOLVED                              |
+----------------------------------------------------------------------+

SOLUTION: Host Auth Inheritance via Volume Mount

CREDENTIAL STORAGE LOCATIONS:
  - macOS/Linux: ~/.claude/
  - Windows: %USERPROFILE%\.claude\
  - Key file: ~/.claude/.credentials.json (OAuth tokens)
  - Config: ~/.claude.json or ~/.claude/settings.json

DOCKER MOUNT STRATEGY:
  docker run -it \
    -v ~/.claude:/root/.claude \
    -v $(pwd):/workspace \
    cortical-agent

  If running as non-root user:
    -v ~/.claude:/home/agentuser/.claude

IMPLEMENTATION NOTES:
  1. User authenticates Claude Code on host ONCE
  2. All containers inherit auth via volume mount
  3. Container needs read/write for token refresh
  4. File-based .credentials.json works reliably
     (not Keychain-dependent)

DOCKERFILE CONSIDERATION:
  # Ensure .claude directory exists with correct permissions
  RUN mkdir -p /home/agentuser/.claude && \
      chown agentuser:agentuser /home/agentuser/.claude

SECURITY NOTE:
  - Mounting credentials gives container full account access
  - Acceptable for our use case (trusted containers)
  - Credentials never committed to git

TODO: Verify similar approach works for:
  [ ] Gemini CLI
  [ ] Codex CLI
  [ ] OpenCode
  (Likely similar ~/.config/ or ~/.<tool>/ patterns)

DECISION: LOCKED - Use host auth inheritance via volume mount

+----------------------------------------------------------------------+
```

### Q2: Naming Decision

```
+----------------------------------------------------------------------+
|              NAMING - RESOLVED                                        |
+----------------------------------------------------------------------+

DECISION: "Protectorate" is the framework/repo name

REPOSITORY:
  github.com/org/protectorate    <- orchestration repo
  github.com/org/cortical-stack  <- memory/stack repo

DIRECTORY STRUCTURE:
  protectorate/
  +-- containers/
  |   +-- agent/     <- Agent container (sleeves)
  |   +-- envoy/     <- Manager container (the coordinator)
  +-- cmd/
  |   +-- envoy/     <- Manager binary entry point
  |   +-- sidecar/   <- Sidecar binary
  +-- internal/
  +-- ...

CLI TOOL NAME: "envoy"
  envoy spawn --repo foo --cli claude-code
  envoy status
  envoy resleeve agent-alice --cli gemini
  envoy kill agent-bob

RATIONALE:
  - "Protectorate" = the governing body (Altered Carbon theme)
  - "Envoy" = the messenger/coordinator process
  - Many CLI tools built into manager, so CLI is "envoy"
  - More evocative and fun naming scheme

UPDATED TERMINOLOGY:
  - Framework: Protectorate
  - Manager process/CLI: Envoy
  - Agent containers: Sleeves
  - Memory format: Cortical Stack

DECISION: LOCKED

+----------------------------------------------------------------------+
```

### Q3: cortical-stack Project Agnosticism

```
+----------------------------------------------------------------------+
|              CORTICAL-STACK: UNIVERSAL DESIGN - LOCKED                |
+----------------------------------------------------------------------+

REQUIREMENT: cortical-stack repo should be usable by ANYONE
             Not just fit-for-only-our-system

IMPLICATIONS:

  1. No protectorate-specific assumptions
     - Stack format should work standalone
     - No required manager integration
     - Clean API that any orchestrator could use

  2. Documentation for independent use
     - "Use with Protectorate" as one option
     - "Use standalone" as equally valid
     - "Use with your own orchestrator" supported

  3. Versioning independence
     - cortical-stack versions not tied to protectorate versions
     - Can adopt cortical-stack without adopting protectorate

  4. Minimal dependencies
     - Pure Go, no protectorate imports
     - Self-contained parsers and writers

  5. Universal markdown format
     - .cstack/ works with any AI CLI tool
     - Human readable/editable
     - No proprietary encoding

DESIGN PRINCIPLE: LOCKED
Stack repo must remain decoupled and universally useful.

+----------------------------------------------------------------------+
```

### Q4: Agent Container Docker Access for Testing

```
+----------------------------------------------------------------------+
|              DOCKER ACCESS FOR AGENT TESTING - RESOLVED               |
+----------------------------------------------------------------------+

PROBLEM: Agent containers working on projects may need Docker
         (e.g., running tests that spawn containers)

SCENARIO: Agent working on a web app needs to:
  - Spin up a test database container
  - Run integration tests against it
  - Clean up test containers

DECISION: Option A (Manager Proxy) for V1

  +------------------+       +------------------+
  | Agent Container  |------>| Envoy Manager    |
  | "spawn postgres" |       | spawns container |
  +------------------+       +------------------+
          |                          |
          v                          v
  +------------------+       +------------------+
  | Sidecar endpoint |       | Docker API call  |
  | POST /docker/run |       | on behalf of     |
  +------------------+       +------------------+

API DESIGN:
  POST /docker/spawn
  {
    "image": "postgres:15",
    "network": "cortical-dev",
    "env": {"POSTGRES_PASSWORD": "test"},
    "ports": ["5432"]
  }

  Response:
  {
    "container_id": "abc123",
    "address": "postgres-abc123.cortical-dev:5432"
  }

WHY OPTION A:
  - Full control and audit trail
  - Manager maintains visibility of all containers
  - Security boundary clear
  - Simplest to implement correctly

FALLBACK OPTIONS (if Option A has issues):

  [OPTION B - Annotated for future consideration]
  Sidecar/Hook Intercept:
    - Agent tries to run docker commands
    - Sidecar intercepts via hook
    - Routes request to manager
    PRO: Transparent to agent
    CON: Complex interception, edge cases
    WHEN TO TRY: If agents struggle with explicit API calls

  [OPTION C - Annotated for future consideration]
  Restricted Docker Socket:
    - Agent has docker socket access
    - BUT can ONLY interact with dev network
    - Three networks: dev, prod, cortical
    PRO: Native docker experience for agent
    CON: Security boundary enforcement complexity
    WHEN TO TRY: If latency becomes problematic

CRITICAL FOR DOGFOODING:
  - We need to work on Protectorate using Protectorate
  - Agents will need to build/test containers
  - This MUST be solved for V1

DECISION: LOCKED - Start with Option A, fallback to B/C if needed

+----------------------------------------------------------------------+
```

### Q5: Pre-installed Languages in Agent Image

```
+----------------------------------------------------------------------+
|              AGENT IMAGE: PRE-INSTALLED TOOLCHAINS - LOCKED           |
+----------------------------------------------------------------------+

DECISION: Include all primary toolchains from the start

LANGUAGES INCLUDED:
  [x] Python3 + pip
  [x] Node.js + npm (required for AI CLIs)
  [x] Bun
  [x] Rust + Cargo
  [x] Zig

DOCKERFILE:
  FROM debian:bookworm-slim

  # System dependencies
  RUN apt-get update && apt-get install -y \
      curl git ca-certificates build-essential \
      python3 python3-pip python3-venv \
      && rm -rf /var/lib/apt/lists/*

  # Node.js (for AI CLI tools)
  RUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
      apt-get install -y nodejs

  # Bun
  RUN curl -fsSL https://bun.sh/install | bash
  ENV PATH="/root/.bun/bin:$PATH"

  # Rust
  RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | \
      sh -s -- -y --default-toolchain stable
  ENV PATH="/root/.cargo/bin:$PATH"

  # Zig
  RUN curl -fsSL https://ziglang.org/download/0.11.0/zig-linux-x86_64-0.11.0.tar.xz | \
      tar -xJ -C /usr/local && \
      ln -s /usr/local/zig-linux-x86_64-0.11.0/zig /usr/local/bin/zig

IMAGE SIZE:
  - Base Debian: ~80MB
  - Python: ~100MB
  - Node.js: ~100MB
  - Bun: ~50MB
  - Rust + Cargo: ~500MB
  - Zig: ~50MB
  - AI CLIs: ~200MB
  - TOTAL: ~1.1GB

RATIONALE:
  SPAWN TIME >> IMAGE SIZE

  - Image pulled once, cached locally
  - Every spawn benefits from pre-installed tools
  - No waiting for "apt-get install rust" on each task
  - Consistent versions across all agents
  - Our projects primarily use these languages

FUTURE CONSIDERATION:
  - May create "slim" variant for resource-constrained hosts
  - But default image includes everything

DECISION: LOCKED - Include Python, Node, Bun, Rust, Zig from start

+----------------------------------------------------------------------+
```

### Q6: One Sleeve Per Stack Policy

```
+----------------------------------------------------------------------+
|              ONE SLEEVE PER STACK - POLICY LOCKED                     |
+----------------------------------------------------------------------+

POLICY: ONE SLEEVE PER STACK (STRICTLY ENFORCED)

We are NOT optimizing for multi-agent per repo.
We DISALLOW multiple sleeves sharing one stack.

RATIONALE (Altered Carbon Canon):
  +----------------------------------------------------------+
  | In Altered Carbon, if two sleeves share a cortical       |
  | stack simultaneously, it gets *weird*. The DHF           |
  | (Digital Human Freight / consciousness) expects          |
  | exclusive access to its stack. Double-sleeving is        |
  | disorienting, illegal, and causes identity fragmentation.|
  +----------------------------------------------------------+

  Technical reality:
  - Concurrent writes to same .cstack/ = corruption
  - Race conditions on CURRENT.md, PLAN.md
  - Conflicting OUTBOX messages
  - No clear "who is doing what"

ENFORCEMENT:
  +------------------+     +------------------+
  | Agent A          |     | Agent B          |
  | repo-foo/.cstack |     | repo-bar/.cstack |
  +------------------+     +------------------+
         |                        |
         v                        v
     ONE sleeve              ONE sleeve
     per stack               per stack

  FORBIDDEN (Manager will reject):
  +------------------+
  | Agent A + B      |  <- REJECTED: Double-sleeving
  | repo-foo/.cstack |
  +------------------+

IF USER WANTS PARALLELISM:

  OPTION 1: Subagents within the CLI harness (PREFERRED)
    +------------------------------------------+
    | Agent Container (Claude Code)            |
    |                                          |
    | Main agent spawns subagents internally:  |
    |   - Subagent A: works on module X        |
    |   - Subagent B: works on module Y        |
    |   - Main agent: coordinates results      |
    |                                          |
    | All within ONE sleeve, ONE stack         |
    +------------------------------------------+

    - Claude Code already supports this
    - Gemini CLI has similar capabilities
    - This is the intended parallelism model

  OPTION 2: Separate repos, separate sleeves
    - Repo A -> Agent A (its own stack)
    - Repo B -> Agent B (its own stack)
    - Both run in parallel, no conflict

BEFORE WE EVER CONSIDER MULTI-SLEEVE-PER-STACK:
  1. Build tooling for agents to spawn subagents
  2. Manager helps coordinate subagent results
  3. Prove single-sleeve model is insufficient
  4. Design explicit locking/coordination if needed

POLICY: LOCKED - One sleeve per stack, no exceptions

+----------------------------------------------------------------------+
```

### Q7: Two Resleeve Options

```
+----------------------------------------------------------------------+
|              RESLEEVE TYPES: SOFT vs HARD - LOCKED                    |
+----------------------------------------------------------------------+

DECISION: Two distinct resleeve operations

TYPE 1: SOFT RESLEEVE (CLI Swap)
  +------------------------------------------+
  | Container stays alive                     |
  | Just swap the AI CLI process             |
  |                                           |
  | tmux session:                             |
  |   [kill claude-code]                      |
  |   [start codex-cli]                       |
  |                                           |
  | Same workspace, same .cstack/             |
  | Just different "consciousness"            |
  +------------------------------------------+

  USE CASE:
    - Quick persona switch
    - "Let me try this with Gemini instead"
    - No container restart overhead
    - Test same task with different AI

  IMPLEMENTATION:
    - Sidecar exposes /resleeve endpoint
    - Accepts CLI name parameter
    - Kills current CLI process in tmux
    - Starts new CLI with same workspace
    - Updates CURRENT.md with new CLI info

TYPE 2: HARD RESLEEVE (Container Respawn)
  +------------------------------------------+
  | Container is destroyed                    |
  | Fresh container spawned                   |
  |                                           |
  | docker kill agent-foo                     |
  | docker run ... agent-foo                  |
  |                                           |
  | Same volume mounts                        |
  | Clean process state                       |
  +------------------------------------------+

  USE CASE:
    - Container got into bad state
    - Need fresh environment
    - Memory issues, zombie processes
    - OS-level corruption

  IMPLEMENTATION:
    - Manager handles via existing spawn/kill
    - Workspace volume persists
    - .cstack/ survives (that's the whole point)

CLI COMMANDS:
  envoy resleeve agent-alice --cli gemini    # soft
  envoy resleeve agent-alice --hard          # hard
  envoy resleeve agent-alice --hard --cli codex  # hard + different CLI

API:
  POST /agent/{id}/resleeve
  {
    "type": "soft",     // or "hard"
    "cli": "codex-cli"  // required for soft, optional for hard
  }

DECISION: LOCKED

+----------------------------------------------------------------------+
```

### Q8: Reverse Proxy for V1?

```
+----------------------------------------------------------------------+
|              TRAEFIK / REVERSE PROXY - DEFERRED TO V2                 |
+----------------------------------------------------------------------+

DECISION: Defer reverse proxy to V2, but research sooner rather than later

V1 APPROACH (Direct Ports):
  - Manager: localhost:7470
  - Gitea: localhost:3000
  - Agent sidecars: localhost:8081, 8082, ...

  This is simple and works for single-machine deployment.

V2 APPROACH (Traefik):
  - manager.cortical.local -> manager:7470
  - gitea.cortical.local -> gitea:3000
  - agent-foo.cortical.local -> agent-foo:8080
  - SSL termination
  - Auto service discovery

WHY DEFER:
  - V1 is single machine, ports are fine
  - Don't add complexity before needed
  - Focus on core functionality first

RESEARCH PRIORITIES (Before V2):
  +----------------------------------------------------------+
  | Research these SOONER rather than later:                  |
  |                                                          |
  | 1. Traefik Docker provider                               |
  |    - Auto-discovers containers via labels                |
  |    - No manual config per agent                          |
  |                                                          |
  | 2. Service discovery patterns                            |
  |    - How agents find each other                          |
  |    - How manager routes to dynamic agents                |
  |                                                          |
  | 3. SSL certificate automation                            |
  |    - Let's Encrypt for public domains                    |
  |    - Self-signed for local/private deployments           |
  |                                                          |
  | 4. Multi-host networking                                 |
  |    - Docker Swarm overlay networks                       |
  |    - Or: Traefik as central router                       |
  +----------------------------------------------------------+

TRAEFIK LABELS (Preview for V2):
  labels:
    - "traefik.enable=true"
    - "traefik.http.routers.agent-${NAME}.rule=Host(`${NAME}.cortical.local`)"
    - "traefik.http.services.agent-${NAME}.loadbalancer.server.port=8080"

DECISION: DEFERRED - Use ports for V1, research proxy patterns early

+----------------------------------------------------------------------+
```

### Q9: Ralphing and Agent Loops

```
+----------------------------------------------------------------------+
|              RALPHING: AGENT LOOP MANAGEMENT - V2 (NEEDS RESEARCH)    |
+----------------------------------------------------------------------+

STATUS: V2 Feature - Requires deep research before implementation

REFERENCE: https://ghuntley.com/loop/
           MUST deep dive this before designing loop architecture

CONCEPT: "Ralphing" = agent running in autonomous loops
         Named after the tendency to... go off the rails

PROJECT ETHOS ADDITION:
  +----------------------------------------------------------+
  | Allow agents to run in loops:                             |
  |   - IN SAFE, ISOLATED ENVIRONMENTS                        |
  |   - With observability for manager and user               |
  |   - Ability to tweak/guide loops as needed                |
  |   - Clear guardrails and kill switches                    |
  +----------------------------------------------------------+

LOOP SAFETY MODEL (Conceptual):
  +--------------------------------------------------+
  |  AGENT LOOP                                       |
  |  +--------------------------------------------+  |
  |  |  Iteration 1 -> Iteration 2 -> ...         |  |
  |  |       ^              ^                      |  |
  |  |       |              |                      |  |
  |  +-------|--------------|---------------------+  |
  |          |              |                        |
  |     OBSERVATION     INTERVENTION                 |
  |     (read-only)     (if needed)                  |
  |          |              |                        |
  |          v              v                        |
  |  +--------------------------------------------+  |
  |  |  ENVOY MANAGER / USER INTERFACE            |  |
  |  |  - View loop progress                      |  |
  |  |  - See iteration outputs                   |  |
  |  |  - Inject guidance                         |  |
  |  |  - Pause/resume loop                       |  |
  |  |  - Set guardrails                          |  |
  |  |  - KILL SWITCH                             |  |
  |  +--------------------------------------------+  |
  +--------------------------------------------------+

GUARDRAILS (To be designed):
  - Max iterations per loop
  - Cost/token budget
  - Time limits
  - Human checkpoint intervals
  - Sandboxed execution environment
  - Automatic pause on anomaly detection

RESEARCH TODO:
  +----------------------------------------------------------+
  | BEFORE implementing loops, we MUST:                       |
  |                                                          |
  | [ ] Deep dive https://ghuntley.com/loop/                 |
  | [ ] Identify patterns applicable to our architecture     |
  | [ ] Define "safe" loop execution model                   |
  | [ ] Design loop state tracking in .cstack/               |
  | [ ] Design manager observation/intervention APIs         |
  | [ ] Define cost/resource monitoring approach             |
  | [ ] Document failure modes and recovery                  |
  +----------------------------------------------------------+

V1 ARCHITECTURE CONSTRAINT:
  - V1 does NOT implement loops
  - BUT V1 architecture MUST NOT preclude loops
  - Stack format should be extensible for loop state
  - Manager API should be extensible for loop control

DECISION: DEFERRED TO V2 - Requires research first

+----------------------------------------------------------------------+
```

### Q10: File/Directory Visualization

```
+----------------------------------------------------------------------+
|              WORKSPACE VISIBILITY MODEL - CONFIRMED                   |
+----------------------------------------------------------------------+

CONFIRMED: This is the correct architecture

ENVOY MANAGER RESPONSIBILITIES:
  1. Clones repos (from Gitea)
  2. Mounts specific repo to specific agent
  3. Each agent gets ONE repo workspace
  4. Maintains read access to all agent workspaces

AGENT CONTAINER VISIBILITY:
  +------------------------------------------+
  | Agent Container (Sleeve)                 |
  |                                          |
  | /workspace/                              |
  |   +-- repo-foo/           <- MOUNTED     |
  |       +-- .cstack/        <- AGENT STATE |
  |       +-- src/                           |
  |       +-- ...                            |
  |                                          |
  | /shared/                  <- MOUNTED     |
  |   +-- arena/              <- SHARED MSGS |
  |                           (V2 feature)   |
  +------------------------------------------+

  Agent CAN see:
    [x] Its own repo (/workspace/repo-foo/)
    [x] Its own .cstack/ (/workspace/repo-foo/.cstack/)
    [x] Shared arena (/shared/arena/) - mounted but unused in V1

  Agent CANNOT see:
    [ ] Other agent repos
    [ ] Other agent .cstack/ directories
    [ ] Manager internals
    [ ] Host filesystem
    [ ] Docker socket (uses manager proxy)

ENVOY MANAGER VISIBILITY:
  +------------------------------------------+
  | Envoy Manager Container                  |
  |                                          |
  | /workspaces/              <- ALL MOUNTED |
  |   +-- agent-alice/                       |
  |   |   +-- repo-foo/                      |
  |   |       +-- .cstack/                   |
  |   +-- agent-bob/                         |
  |   |   +-- repo-bar/                      |
  |   |       +-- .cstack/                   |
  |   +-- agent-carol/                       |
  |       +-- repo-baz/                      |
  |           +-- .cstack/                   |
  |                                          |
  | /shared/                                 |
  |   +-- arena/              <- MANAGED     |
  |                                          |
  | Manager operations:                      |
  |   - grep/qmd across all .cstack/         |
  |   - Read any agent's state               |
  |   - Route messages between agents        |
  |   - Aggregate status across fleet        |
  |   - Write to shared arena                |
  +------------------------------------------+

ARCHITECTURE DIAGRAM:
                    +------------------------+
                    |     ENVOY MANAGER      |
                    | /workspaces/* (R/W)    |
                    | /shared/arena/ (R/W)   |
                    | Docker socket access   |
                    +----------+-------------+
                               |
         +---------------------+---------------------+
         |                     |                     |
         v                     v                     v
  +------+--------+     +------+--------+     +------+--------+
  | Agent Alice   |     | Agent Bob     |     | Agent Carol   |
  | /workspace/   |     | /workspace/   |     | /workspace/   |
  |   repo-foo/   |     |   repo-bar/   |     |   repo-baz/   |
  |     .cstack/  |     |     .cstack/  |     |     .cstack/  |
  | /shared/arena |     | /shared/arena |     | /shared/arena |
  +---------------+     +---------------+     +---------------+
   (isolated)            (isolated)            (isolated)

SHARED ARENA (/shared/arena/):
  +----------------------------------------------------------+
  | Mounted in V1 but NOT USED until V2                       |
  |                                                          |
  | Future uses:                                             |
  |   - Global broadcast messages from manager               |
  |   - Shared knowledge base / context                      |
  |   - Cross-agent announcements                            |
  |   - Manager moderates all writes                         |
  |                                                          |
  | Why mount now:                                           |
  |   - Avoids container definition changes later            |
  |   - Ready when we need it                                |
  +----------------------------------------------------------+

DECISION: CONFIRMED - Architecture is correct, keep /shared/arena/

+----------------------------------------------------------------------+
```

---

## Document History

| Date | Version | Changes |
|------|---------|---------|
| 2026-01-19 | 2.2 | Resolved Q1-Q10: Auth, naming (Protectorate), languages, policies |
| 2026-01-19 | 2.1 | Added Open Questions section (Q1-Q10) |
| 2026-01-19 | 2.0 | Complete rewrite with locked decisions |
| 2026-01-16 | 1.0 | Initial options document |

---

*All decisions are now locked. Ready for implementation.*
